{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7ps4zdKnm8N"
      },
      "source": [
        "# Task\n",
        "Obtener, limpiar y unificar datos meteorológicos de Meteoblue (usando la clave API \"API_KEY\"), SIATA e IDEAM RADAR para Medellín y su Área Metropolitana. Posteriormente, almacenar los datos procesados ​​y proporcionar un resumen del desarrollo, sugerencias para la creación de un panel de control y pasos prácticos de programación adicionales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8lKpXo0nxka"
      },
      "source": [
        "### Investigación de Fuentes de Datos: SIATA\n",
        "\n",
        "Para investigar las APIs o métodos de acceso a datos de SIATA (Sistema de Alerta Temprana de Medellín y el Valle de Aburrá), se recomienda seguir los siguientes pasos:\n",
        "\n",
        "1.  **Visitar el sitio web oficial de SIATA:** Busca secciones dedicadas a 'Desarrolladores', 'APIs', 'Datos Abiertos' o 'Documentación Técnica'.\n",
        "2.  **Consultar la documentación de la API:** Si encuentras una API, revisa cuidadosamente su documentación para entender:\n",
        "    *   Los _endpoints_ disponibles.\n",
        "    *   Los tipos de datos que ofrece (sensores meteorológicos, calidad del aire, hidrología, etc.).\n",
        "    *   El formato de los datos (JSON, XML, CSV).\n",
        "    *   Requisitos de autenticación o claves de API.\n",
        "    *   Límites de tasa (rate limits) o restricciones de uso.\n",
        "3.  **Identificar el estado de las secciones:** Presta especial atención a cualquier aviso sobre secciones fuera de servicio o información desactualizada, como se mencionó en la descripción de la tarea.\n",
        "4.  **Explorar alternativas (si es necesario):** Si las APIs principales no son funcionales o no proporcionan los datos necesarios, considera:\n",
        "    *   **Repositorios de datos abiertos:** Muchos gobiernos locales y entidades ofrecen portales de datos abiertos donde se publican conjuntos de datos históricos o en tiempo casi real. Busca el portal de datos abiertos de Medellín o Antioquia.\n",
        "    *   **Web Scraping (con precaución):** Si no hay una API disponible y los datos están públicamente accesibles en el sitio web de SIATA de forma estructurada, el web scraping podría ser una opción. Sin embargo, esto debe hacerse con ética, respetando los términos de servicio del sitio web y asegurando que no se sobrecargue el servidor. Esta es una opción de último recurso y requiere consideración legal y ética.\n",
        "    *   **Contacto directo:** En algunos casos, contactar directamente a SIATA para solicitar acceso a datos o información sobre APIs puede ser una vía.\n",
        "\n",
        "Documentaremos todos los hallazgos, incluyendo la disponibilidad, formato, autenticación y limitaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46LPL_J4qQyP"
      },
      "source": [
        "## Descargar y Procesar Datos de SIATA (Archivo)\n",
        "\n",
        "### Subtask:\n",
        "Acceder a `https://www.siata.gov.co/operacional/` para identificar y descargar un archivo de datos relevantes. Se implementará la lógica para leer este archivo, limpiar los datos de posibles errores y prepararlos para su integración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP0az9zHqVn2",
        "outputId": "1cdd1121-3c54-4e7e-d49c-01ac285c0dac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch data from SIATA operational URL: https://www.siata.gov.co/operacional/\n",
            "Successfully fetched SIATA operational page from https://www.siata.gov.co/operacional/.\n",
            "Title of the page: Listing folder\n",
            "HTML content parsed successfully.\n",
            "\n",
            "No direct downloadable data links (.csv, .xlsx, .json, etc.) found on the page.\n",
            "Manual inspection of the page or network requests might be necessary to locate the data source.\n",
            "Failed to download SIATA data or no direct download links found.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# 1. Define the URL for SIATA's operational data section\n",
        "SIATA_OPERACIONAL_URL = \"https://www.siata.gov.co/operacional/\"\n",
        "\n",
        "print(f\"Attempting to fetch data from SIATA operational URL: {SIATA_OPERACIONAL_URL}\")\n",
        "\n",
        "siata_operacional_soup = None\n",
        "siata_downloaded_data = None # Variable to store raw file content if downloaded\n",
        "\n",
        "try:\n",
        "    # 2. Make an HTTP GET request to the URL, allowing redirects and including a timeout\n",
        "    response = requests.get(SIATA_OPERACIONAL_URL, allow_redirects=True, timeout=20)\n",
        "    # 3. Implement robust error handling\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Successfully fetched SIATA operational page from {SIATA_OPERACIONAL_URL}.\")\n",
        "\n",
        "    # 4. Parse the HTML content of the response\n",
        "    siata_operacional_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(f\"Title of the page: {siata_operacional_soup.title.string if siata_operacional_soup.title else 'No title found'}\")\n",
        "    print(\"HTML content parsed successfully.\")\n",
        "\n",
        "    # 5. Inspect the parsed HTML to identify potential links to downloadable data files\n",
        "    # This is a placeholder for detailed inspection. We'll look for common download link patterns.\n",
        "    download_links = []\n",
        "    for link in siata_operacional_soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        # Look for common file extensions or keywords indicating data downloads\n",
        "        if any(ext in href for ext in ['.csv', '.xlsx', '.json', '.zip', '.txt', 'download', 'export', 'data']):\n",
        "            # Construct absolute URL if it's relative\n",
        "            if not href.startswith('http'):\n",
        "                # Simple join for relative paths, more complex logic needed for all cases\n",
        "                absolute_href = requests.compat.urljoin(SIATA_OPERACIONAL_URL, href)\n",
        "            else:\n",
        "                absolute_href = href\n",
        "            download_links.append(absolute_href)\n",
        "\n",
        "    if download_links:\n",
        "        print(\"\\nPotential downloadable data links found:\")\n",
        "        for dl_link in download_links:\n",
        "            print(f\"- {dl_link}\")\n",
        "            # 6. For simplicity, attempt to download the first identified relevant file for now\n",
        "            # In a real scenario, we might need to filter more specifically or present options.\n",
        "            if siata_downloaded_data is None: # Only download the first one for initial check\n",
        "                print(f\"Attempting to download data from: {dl_link}\")\n",
        "                data_file_response = requests.get(dl_link, allow_redirects=True, timeout=20)\n",
        "                data_file_response.raise_for_status()\n",
        "                siata_downloaded_data = data_file_response.content # Store raw bytes\n",
        "                print(f\"Successfully downloaded data from {dl_link}. Size: {len(siata_downloaded_data)} bytes.\")\n",
        "                # Decide if we want to break after first download or list all and then pick.\n",
        "                # For this step, let's download the first and stop.\n",
        "                # If the content type is text/json, we might try to decode it.\n",
        "                if 'json' in data_file_response.headers.get('Content-Type', ''):\n",
        "                    try:\n",
        "                        siata_downloaded_data = data_file_response.json() # Try parsing as JSON\n",
        "                        print(\"Downloaded data appears to be JSON.\")\n",
        "                    except json.JSONDecodeError:\n",
        "                        print(\"Downloaded data is not valid JSON, storing as raw content.\")\n",
        "                elif 'text' in data_file_response.headers.get('Content-Type', '') or 'csv' in data_file_response.headers.get('Content-Type', ''):\n",
        "                    try:\n",
        "                        siata_downloaded_data = data_file_response.text # Store as text for potential CSV/text parsing\n",
        "                        print(\"Downloaded data appears to be text/CSV.\")\n",
        "                    except Exception as parse_e:\n",
        "                        print(f\"Error parsing downloaded text: {parse_e}\")\n",
        "                break # Only download the first for now\n",
        "    else:\n",
        "        # 7. If no direct downloadable file link is found\n",
        "        print(\"\\nNo direct downloadable data links (.csv, .xlsx, .json, etc.) found on the page.\")\n",
        "        print(\"Manual inspection of the page or network requests might be necessary to locate the data source.\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {SIATA_OPERACIONAL_URL} timed out after 20 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data from SIATA operational URL: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")\n",
        "\n",
        "if siata_downloaded_data is not None:\n",
        "    print(\"SIATA data successfully downloaded and stored in 'siata_downloaded_data'.\")\n",
        "else:\n",
        "    print(\"Failed to download SIATA data or no direct download links found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuuouGNDq_TK"
      },
      "source": [
        "## Analizar el directorio de la página operativa de SIATA\n",
        "### Subtask:\n",
        "Analizar el contenido HTML de la página operativa de SIATA (`https://www.siata.gov.co/operacional/`) para extraer todos los enlaces disponibles, luego identificar y listar los posibles archivos de datos o subdirectorios dentro de este directorio para su posterior análisis.\n",
        "\n",
        "**Reasoning**:\n",
        "El paso anterior recuperó y analizó correctamente la página operativa de SIATA. Ahora, analizaré el objeto `siata_operacional_soup` para extraer todos los enlaces, categorizarlos en «Archivos de datos» y «Subdirectorios» según los criterios especificados e imprimiré un resumen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl012GN_ql1Q",
        "outputId": "712db135-42ee-4d07-acaa-700b26efc2c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analyzing SIATA operational page for links...\n",
            "\n",
            "--- Link Analysis Summary ---\n",
            "No obvious data files found.\n",
            "\n",
            "Found 32 potential Subdirectories:\n",
            "- http://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/operacional/\n",
            "- https://www.siata.gov.co/operacional/?C=M;O=A\n",
            "- https://www.siata.gov.co/operacional/?C=N;O=A\n",
            "- https://www.siata.gov.co/operacional/?C=S;O=A\n",
            "- https://www.siata.gov.co/operacional/CicloAnual/\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/\n",
            "- https://www.siata.gov.co/operacional/WRF/\n",
            "- https://www.siata.gov.co/operacional/enso/\n",
            "- https://www.siata.gov.co/operacional/mapas/\n",
            "- https://www.siata.gov.co/operacional/monitoreo/\n",
            "- https://www.siata.gov.co/operacional/prcSirena/\n",
            "- https://www.siata.gov.co/operacional/radar/\n",
            "- https://www.siata.gov.co/operacional/radiometro/\n",
            "- https://www.siata.gov.co/operacional/seriesdetiempo/\n",
            "- https://www.siata.gov.co/operacional/seriesdetiempo_prueba/\n",
            "\n",
            "Link categorization complete.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "# Assuming siata_operacional_soup is available from the previous step\n",
        "# And SIATA_OPERACIONAL_URL is also available\n",
        "\n",
        "if siata_operacional_soup is None:\n",
        "    print(\"Error: siata_operacional_soup is not available. Please ensure the previous step ran successfully.\")\n",
        "else:\n",
        "    print(\"Analyzing SIATA operational page for links...\")\n",
        "\n",
        "    data_files = []\n",
        "    subdirectories = []\n",
        "    other_links = []\n",
        "\n",
        "    # Common data file extensions and keywords\n",
        "    data_extensions = ['.csv', '.xlsx', '.json', '.zip', '.txt', '.xml', '.kmz', '.tgz', '.gz']\n",
        "    data_keywords = ['data', 'export', 'archivo'] # in file name part\n",
        "\n",
        "    for link in siata_operacional_soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        absolute_href = requests.compat.urljoin(SIATA_OPERACIONAL_URL, href)\n",
        "\n",
        "        # Extract the path part of the URL to check for extensions/keywords\n",
        "        parsed_url = urlparse(absolute_href)\n",
        "        path = parsed_url.path.lower()\n",
        "\n",
        "        # 3. Categorize links\n",
        "        if path.endswith('/'):\n",
        "            subdirectories.append(absolute_href)\n",
        "        elif any(path.endswith(ext) for ext in data_extensions) or any(keyword in path for keyword in data_keywords):\n",
        "            data_files.append(absolute_href)\n",
        "        else:\n",
        "            other_links.append(absolute_href)\n",
        "\n",
        "    print(\"\\n--- Link Analysis Summary ---\")\n",
        "\n",
        "    if data_files:\n",
        "        print(f\"Found {len(data_files)} potential Data Files:\")\n",
        "        for df_link in sorted(list(set(data_files))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {df_link}\")\n",
        "    else:\n",
        "        print(\"No obvious data files found.\")\n",
        "\n",
        "    if subdirectories:\n",
        "        print(f\"\\nFound {len(subdirectories)} potential Subdirectories:\")\n",
        "        for sub_link in sorted(list(set(subdirectories))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {sub_link}\")\n",
        "    else:\n",
        "        print(\"No obvious subdirectories found.\")\n",
        "\n",
        "    # Optionally, print other links for full transparency\n",
        "    # if other_links:\n",
        "    #     print(f\"\\nFound {len(other_links)} other links (not categorized as data files or subdirectories):\")\n",
        "    #     for other_link_item in sorted(list(set(other_links))):\n",
        "    #         print(f\"- {other_link_item}\")\n",
        "\n",
        "    # Store the categorized links for potential future steps if needed\n",
        "    siata_found_data_files = list(set(data_files))\n",
        "    siata_found_subdirectories = list(set(subdirectories))\n",
        "\n",
        "    print(\"\\nLink categorization complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAHEEPXtrpa1",
        "outputId": "5a6404b6-1bd7-417d-d3e0-c89c73682bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch data from SIATA Meteorological URL: https://www.siata.gov.co/operacional/Meteorologia/\n",
            "Successfully fetched SIATA Meteorological page from https://www.siata.gov.co/operacional/Meteorologia/.\n",
            "Title of the page: Listing folder\n",
            "HTML content parsed successfully.\n",
            "\n",
            "--- Link Analysis Summary for Meteorologia Directory ---\n",
            "No obvious data files found in this directory.\n",
            "\n",
            "Found 11 potential Subdirectories:\n",
            "- http://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/operacional/\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/?C=M;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/?C=N;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/?C=S;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# 1. Define the URL for SIATA's Meteorological subdirectory\n",
        "SIATA_METEOROLOGIA_URL = \"https://www.siata.gov.co/operacional/Meteorologia/\"\n",
        "\n",
        "print(f\"Attempting to fetch data from SIATA Meteorological URL: {SIATA_METEOROLOGIA_URL}\")\n",
        "\n",
        "siata_meteorologia_soup = None\n",
        "\n",
        "try:\n",
        "    # 2. Make an HTTP GET request to the URL, allowing redirects and including a timeout\n",
        "    response = requests.get(SIATA_METEOROLOGIA_URL, allow_redirects=True, timeout=20)\n",
        "    # 3. Implement robust error handling\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Successfully fetched SIATA Meteorological page from {SIATA_METEOROLOGIA_URL}.\")\n",
        "\n",
        "    # 4. Parse the HTML content of the response\n",
        "    siata_meteorologia_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(f\"Title of the page: {siata_meteorologia_soup.title.string if siata_meteorologia_soup.title else 'No title found'}\")\n",
        "    print(\"HTML content parsed successfully.\")\n",
        "\n",
        "    # 5. Extract all links found on the page.\n",
        "    data_files = []\n",
        "    subdirectories = []\n",
        "\n",
        "    # Common data file extensions and keywords\n",
        "    data_extensions = ['.csv', '.xlsx', '.json', '.zip', '.txt', '.xml', '.kmz', '.tgz', '.gz']\n",
        "    data_keywords = ['data', 'export', 'archivo'] # in file name part\n",
        "\n",
        "    for link in siata_meteorologia_soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        absolute_href = urljoin(SIATA_METEOROLOGIA_URL, href)\n",
        "\n",
        "        # Extract the path part of the URL to check for extensions/keywords\n",
        "        parsed_url = urlparse(absolute_href)\n",
        "        path = parsed_url.path.lower()\n",
        "\n",
        "        # 6. Categorize the extracted links\n",
        "        if path.endswith('/') and absolute_href != SIATA_METEOROLOGIA_URL and absolute_href != SIATA_METEOROLOGIA_URL + '../': # Exclude current dir and parent dir\n",
        "            subdirectories.append(absolute_href)\n",
        "        elif any(path.endswith(ext) for ext in data_extensions) or any(keyword in path for keyword in data_keywords):\n",
        "            data_files.append(absolute_href)\n",
        "\n",
        "    # 7. Print a summary of the findings\n",
        "    print(\"\\n--- Link Analysis Summary for Meteorologia Directory ---\")\n",
        "\n",
        "    if data_files:\n",
        "        print(f\"Found {len(data_files)} potential Data Files:\")\n",
        "        for df_link in sorted(list(set(data_files))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {df_link}\")\n",
        "    else:\n",
        "        print(\"No obvious data files found in this directory.\")\n",
        "\n",
        "    if subdirectories:\n",
        "        print(f\"\\nFound {len(subdirectories)} potential Subdirectories:\")\n",
        "        for sub_link in sorted(list(set(subdirectories))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {sub_link}\")\n",
        "    else:\n",
        "        print(\"No obvious subdirectories found in this directory.\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {SIATA_METEOROLOGIA_URL} timed out after 20 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data from SIATA Meteorological URL: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_Et-q-5rt0R",
        "outputId": "cbd93bf1-17af-4d70-89a7-c73a9c6ede1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch data from SIATA Accumulative Precipitation URL: https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/\n",
            "Successfully fetched SIATA Accumulative Precipitation page from https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/.\n",
            "Title of the page: Listing folder\n",
            "HTML content parsed successfully.\n",
            "\n",
            "--- Link Analysis Summary for AcumPrecipitacion Directory ---\n",
            "Found 20 potential Data Files:\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Abril2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Agosto2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Febrero2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Julio2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Junio2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Marzo2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Mayo2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Noviembre2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Octubre2025.txt\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Septiembre2025.txt\n",
            "\n",
            "Found 11 potential Subdirectories:\n",
            "- http://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/?C=M;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/?C=N;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/?C=S;O=A\n",
            "- https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/ReporteMunicipios/\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "# 1. Define the URL for SIATA's Accumulative Precipitation subdirectory\n",
        "SIATA_ACUMPRECIPITACION_URL = \"https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/\"\n",
        "\n",
        "print(f\"Attempting to fetch data from SIATA Accumulative Precipitation URL: {SIATA_ACUMPRECIPITACION_URL}\")\n",
        "\n",
        "siata_acumprecipitacion_soup = None\n",
        "\n",
        "try:\n",
        "    # 2. Make an HTTP GET request to the URL, allowing redirects and including a timeout\n",
        "    response = requests.get(SIATA_ACUMPRECIPITACION_URL, allow_redirects=True, timeout=20)\n",
        "    # 3. Implement robust error handling\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Successfully fetched SIATA Accumulative Precipitation page from {SIATA_ACUMPRECIPITACION_URL}.\")\n",
        "\n",
        "    # 4. Parse the HTML content of the response\n",
        "    siata_acumprecipitacion_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(f\"Title of the page: {siata_acumprecipitacion_soup.title.string if siata_acumprecipitacion_soup.title else 'No title found'}\")\n",
        "    print(\"HTML content parsed successfully.\")\n",
        "\n",
        "    # 5. Extract all links found on the page.\n",
        "    data_files = []\n",
        "    subdirectories = []\n",
        "\n",
        "    # Common data file extensions and keywords\n",
        "    data_extensions = ['.csv', '.xlsx', '.json', '.zip', '.txt', '.xml', '.kmz', '.tgz', '.gz']\n",
        "    data_keywords = ['data', 'export', 'archivo'] # in file name part\n",
        "\n",
        "    for link in siata_acumprecipitacion_soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        absolute_href = urljoin(SIATA_ACUMPRECIPITACION_URL, href)\n",
        "\n",
        "        # Extract the path part of the URL to check for extensions/keywords\n",
        "        parsed_url = urlparse(absolute_href)\n",
        "        path = parsed_url.path.lower()\n",
        "\n",
        "        # 6. Categorize the extracted links\n",
        "        if path.endswith('/') and absolute_href != SIATA_ACUMPRECIPITACION_URL and absolute_href != SIATA_ACUMPRECIPITACION_URL + '../': # Exclude current dir and parent dir\n",
        "            subdirectories.append(absolute_href)\n",
        "        elif any(path.endswith(ext) for ext in data_extensions) or any(keyword in path for keyword in data_keywords):\n",
        "            data_files.append(absolute_href)\n",
        "\n",
        "    # 7. Print a summary of the findings\n",
        "    print(\"\\n--- Link Analysis Summary for AcumPrecipitacion Directory ---\")\n",
        "\n",
        "    if data_files:\n",
        "        print(f\"Found {len(data_files)} potential Data Files:\")\n",
        "        for df_link in sorted(list(set(data_files))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {df_link}\")\n",
        "    else:\n",
        "        print(\"No obvious data files found in this directory.\")\n",
        "\n",
        "    if subdirectories:\n",
        "        print(f\"\\nFound {len(subdirectories)} potential Subdirectories:\")\n",
        "        for sub_link in sorted(list(set(subdirectories))): # Use set to avoid duplicates and sort for readability\n",
        "            print(f\"- {sub_link}\")\n",
        "    else:\n",
        "        print(\"No obvious subdirectories found in this directory.\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {SIATA_ACUMPRECIPITACION_URL} timed out after 20 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data from SIATA Accumulative Precipitation URL: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b99732e",
        "outputId": "aa992b42-1e68-4df0-e6ee-48175ded4ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to download data file from: https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Noviembre2025.txt\n",
            "Successfully downloaded data from https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Noviembre2025.txt.\n",
            "\n",
            "--- Sample of raw file content (first 500 characters) ---\n",
            "Fecha actualizacion: 2025/11/21 00:01\n",
            "Estacion,Nombre,Municipio,Barrio,Climatologia mes,Acumulado Mes (mm),Porcentaje Mes\n",
            "420, Pueblo Viejo - Pluviometro, La Estrella, NULL, 172.720, 318.770, 184.559\n",
            "66, I.E San Andres (Sede El Socorro), Girardota, NA, 120.730, 217.424, 180.091\n",
            "62, Gimnasio Cantabria, La Estrella, NA, 208.720, 308.356, 147.737\n",
            "248, Vivero EPM Piedras Blancas - Pluviometro, Guarne, NA, 190.500, 276.352, 145.067\n",
            "127, I.E. Manuel Jose Sierra - Sede la Holanda, Girardota, NA, 146.71\n",
            "-----------------------------------------------------------\n",
            "\n",
            "Successfully parsed data into a DataFrame (assuming tab-separated).\n",
            "First 5 rows of SIATA precipitation DataFrame:\n",
            "               Fecha actualizacion: 2025/11/21 00:01\n",
            "0  Estacion,Nombre,Municipio,Barrio,Climatologia ...\n",
            "1  420, Pueblo Viejo - Pluviometro, La Estrella, ...\n",
            "2  66, I.E San Andres (Sede El Socorro), Girardot...\n",
            "3  62, Gimnasio Cantabria, La Estrella, NA, 208.7...\n",
            "4  248, Vivero EPM Piedras Blancas - Pluviometro,...\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 126 entries, 0 to 125\n",
            "Data columns (total 1 columns):\n",
            " #   Column                                 Non-Null Count  Dtype \n",
            "---  ------                                 --------------  ----- \n",
            " 0   Fecha actualizacion: 2025/11/21 00:01  126 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 1.1+ KB\n",
            "SIATA precipitation data successfully loaded into 'siata_precipitacion_data' DataFrame.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import io\n",
        "\n",
        "# URL of a specific data file identified in the previous step\n",
        "# Choosing DatosPacum_Noviembre2025.txt as an example\n",
        "SIATA_DATA_FILE_URL = \"https://www.siata.gov.co/operacional/Meteorologia/AcumPrecipitacion/DatosPacum_Noviembre2025.txt\"\n",
        "\n",
        "print(f\"Attempting to download data file from: {SIATA_DATA_FILE_URL}\")\n",
        "\n",
        "siata_precipitacion_data = None\n",
        "\n",
        "try:\n",
        "    # Make an HTTP GET request to download the file\n",
        "    response = requests.get(SIATA_DATA_FILE_URL, allow_redirects=True, timeout=30)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(f\"Successfully downloaded data from {SIATA_DATA_FILE_URL}.\")\n",
        "\n",
        "    # The content is text, likely tab-separated based on typical SIATA files or simple text\n",
        "    # Let's try to read it into a pandas DataFrame, first as a raw string to inspect.\n",
        "    raw_content = response.text\n",
        "    print(\"\\n--- Sample of raw file content (first 500 characters) ---\")\n",
        "    print(raw_content[:500])\n",
        "    print(\"-----------------------------------------------------------\")\n",
        "\n",
        "    # Attempt to parse as CSV/TXT, assuming tab or space separated for now\n",
        "    # Given the output format, it's highly likely to be tab-separated or fixed-width.\n",
        "    # Let's try reading with pandas, inferring delimiter or using common ones.\n",
        "    try:\n",
        "        # Using io.StringIO to treat the string content as a file\n",
        "        siata_precipitacion_data = pd.read_csv(io.StringIO(raw_content), sep='\\t', encoding='utf-8')\n",
        "        print(\"\\nSuccessfully parsed data into a DataFrame (assuming tab-separated).\")\n",
        "        print(\"First 5 rows of SIATA precipitation DataFrame:\")\n",
        "        print(siata_precipitacion_data.head())\n",
        "        print(\"\\nDataFrame Info:\")\n",
        "        siata_precipitacion_data.info()\n",
        "    except Exception as parse_error:\n",
        "        print(f\"Error parsing data into DataFrame: {parse_error}\")\n",
        "        print(\"Attempting to read with space as delimiter.\")\n",
        "        try:\n",
        "            siata_precipitacion_data = pd.read_csv(io.StringIO(raw_content), sep=' ', skipinitialspace=True, encoding='utf-8')\n",
        "            print(\"\\nSuccessfully parsed data into a DataFrame (assuming space-separated).\")\n",
        "            print(\"First 5 rows of SIATA precipitation DataFrame:\")\n",
        "            print(siata_precipitacion_data.head())\n",
        "            print(\"\\nDataFrame Info:\")\n",
        "            siata_precipitacion_data.info()\n",
        "        except Exception as second_parse_error:\n",
        "            print(f\"Error parsing data with space delimiter: {second_parse_error}\")\n",
        "            print(\"Could not parse data into DataFrame. Raw content stored as string.\")\n",
        "            siata_precipitacion_data = raw_content # Fallback to raw string if parsing fails\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {SIATA_DATA_FILE_URL} timed out after 30 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data file from SIATA: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")\n",
        "\n",
        "if siata_precipitacion_data is not None and isinstance(siata_precipitacion_data, pd.DataFrame):\n",
        "    print(\"SIATA precipitation data successfully loaded into 'siata_precipitacion_data' DataFrame.\")\n",
        "elif siata_precipitacion_data is not None:\n",
        "    print(\"SIATA precipitation data downloaded but not parsed into DataFrame. Stored as raw content.\")\n",
        "else:\n",
        "    print(\"Failed to download or process SIATA precipitation data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3kOiJLRKr4xY",
        "outputId": "5b52d48d-8e87-4b07-b055-cbb70f828c45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to re-parse SIATA precipitation data with correct delimiter and skipping header.\n",
            "\n",
            "Successfully parsed data into a DataFrame with correct delimiter and skipped header.\n",
            "First 5 rows of cleaned SIATA precipitation DataFrame:\n",
            "   Estacion                                      Nombre     Municipio Barrio  \\\n",
            "0       420                  Pueblo Viejo - Pluviometro   La Estrella   NULL   \n",
            "1        66            I.E San Andres (Sede El Socorro)     Girardota     NA   \n",
            "2        62                          Gimnasio Cantabria   La Estrella     NA   \n",
            "3       248    Vivero EPM Piedras Blancas - Pluviometro        Guarne     NA   \n",
            "4       127   I.E. Manuel Jose Sierra - Sede la Holanda     Girardota     NA   \n",
            "\n",
            "   Climatologia mes  Acumulado Mes (mm)  Porcentaje Mes  \n",
            "0            172.72             318.770         184.559  \n",
            "1            120.73             217.424         180.091  \n",
            "2            208.72             308.356         147.737  \n",
            "3            190.50             276.352         145.067  \n",
            "4            146.71             211.074         143.872  \n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 125 entries, 0 to 124\n",
            "Data columns (total 7 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   Estacion            125 non-null    int64  \n",
            " 1   Nombre              125 non-null    object \n",
            " 2   Municipio           125 non-null    object \n",
            " 3   Barrio              125 non-null    object \n",
            " 4   Climatologia mes    125 non-null    float64\n",
            " 5   Acumulado Mes (mm)  125 non-null    float64\n",
            " 6   Porcentaje Mes      125 non-null    float64\n",
            "dtypes: float64(3), int64(1), object(3)\n",
            "memory usage: 7.0+ KB\n",
            "SIATA precipitation data successfully loaded into 'siata_precipitacion_data' DataFrame.\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming raw_content is available from the previous step\n",
        "# It contains the entire text content of the downloaded file.\n",
        "\n",
        "if raw_content:\n",
        "    print(\"Attempting to re-parse SIATA precipitation data with correct delimiter and skipping header.\")\n",
        "    try:\n",
        "        # Use io.StringIO to treat the string content as a file\n",
        "        # Skip the first row (metadata) using skiprows=1\n",
        "        # Use comma as the delimiter\n",
        "        siata_precipitacion_data_cleaned = pd.read_csv(\n",
        "            io.StringIO(raw_content),\n",
        "            sep=',',\n",
        "            skiprows=1, # Skip the \"Fecha actualizacion\" line\n",
        "            encoding='utf-8',\n",
        "            # We might have issues with extra spaces or inconsistent delimiters, let's refine this if needed\n",
        "            # For now, assuming standard CSV format after skipping the first line.\n",
        "            on_bad_lines='skip' # Skip lines that have too many fields\n",
        "        )\n",
        "        print(\"\\nSuccessfully parsed data into a DataFrame with correct delimiter and skipped header.\")\n",
        "        print(\"First 5 rows of cleaned SIATA precipitation DataFrame:\")\n",
        "        print(siata_precipitacion_data_cleaned.head())\n",
        "        print(\"\\nDataFrame Info:\")\n",
        "        siata_precipitacion_data_cleaned.info()\n",
        "\n",
        "        siata_precipitacion_data = siata_precipitacion_data_cleaned # Update the main variable\n",
        "\n",
        "    except Exception as parse_error:\n",
        "        print(f\"Error re-parsing data into DataFrame: {parse_error}\")\n",
        "        print(\"Could not parse data into DataFrame after cleaning attempt. Raw content stored as string.\")\n",
        "        # Fallback to raw string if parsing fails even after cleaning attempts\n",
        "        siata_precipitacion_data = raw_content\n",
        "else:\n",
        "    print(\"No raw content available for re-parsing.\")\n",
        "\n",
        "if siata_precipitacion_data is not None and isinstance(siata_precipitacion_data, pd.DataFrame):\n",
        "    print(\"SIATA precipitation data successfully loaded into 'siata_precipitacion_data' DataFrame.\")\n",
        "elif siata_precipitacion_data is not None:\n",
        "    print(\"SIATA precipitation data downloaded but not parsed into DataFrame. Stored as raw content.\")\n",
        "else:\n",
        "    print(\"Failed to download or process SIATA precipitation data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8OHyMMCn2Dc"
      },
      "source": [
        "### Investigación de Fuentes de Datos: IDEAM RADAR\n",
        "\n",
        "Para investigar las APIs o métodos de acceso a datos de IDEAM RADAR, especialmente para la región de Medellín, se recomienda seguir los siguientes pasos:\n",
        "\n",
        "1.  **Visitar el sitio web oficial de IDEAM:** Busca secciones dedicadas a 'Radar Meteorológico', 'Datos Abiertos', 'Servicios' o 'API'.\n",
        "2.  **Identificar el acceso a datos de radar:** El IDEAM es la autoridad meteorológica nacional en Colombia. Su sitio web o portales asociados suelen ofrecer acceso a información de radar. Busca específicamente datos de radar para la región de Antioquia o, más específicamente, Medellín.\n",
        "3.  **Consultar la documentación de la API (si existe):** Si IDEAM proporciona una API para datos de radar, revisa su documentación para entender:\n",
        "    *   Los _endpoints_ para datos de radar.\n",
        "    *   El formato de los datos (imágenes, datos geoespaciales como GeoTIFF, NetCDF, etc.).\n",
        "    *   La resolución espacial y temporal de los datos.\n",
        "    *   Requisitos de autenticación o claves de API.\n",
        "    *   Cualquier restricción de uso o límite de tasa.\n",
        "4.  **Explorar portales de datos abiertos o geoservicios:** El IDEAM a menudo comparte sus datos a través de portales de datos abiertos gubernamentales o geoservicios (como WMS, WCS) que permiten el acceso a capas de información geográfica, incluyendo datos de radar.\n",
        "5.  **Contacto directo:** Si la información no es clara o no se encuentra una API adecuada, contactar directamente al IDEAM puede ser necesario para obtener información sobre el acceso programático o la descarga de datos de radar.\n",
        "\n",
        "Documentaremos todos los hallazgos, incluyendo la disponibilidad de datos de radar para Medellín, su formato, los requisitos de autenticación (si los hay) y cualquier limitación de uso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f00030a4",
        "outputId": "5b138b43-85df-43b6-e5ed-211856c55c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch content from IDEAM Radar Viewer URL: http://www.ideam.gov.co/web/tiempo-y-clima/radar-meteorologico\n",
            "Error fetching IDEAM Radar Viewer page: 404 Client Error: Not Found for url: http://www.ideam.gov.co/web/tiempo-y-clima/radar-meteorologico\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# 1. Define the URL for the dedicated IDEAM radar viewer page\n",
        "IDEAM_RADAR_VIEWER_URL = \"http://www.ideam.gov.co/web/tiempo-y-clima/radar-meteorologico\"\n",
        "\n",
        "print(f\"Attempting to fetch content from IDEAM Radar Viewer URL: {IDEAM_RADAR_VIEWER_URL}\")\n",
        "\n",
        "ideam_radar_viewer_soup = None\n",
        "radar_image_urls = []\n",
        "\n",
        "# Keywords to look for in src attributes or surrounding text\n",
        "image_keywords = ['radar', 'imagen', 'animacion', 'actual', 'tiempo', 'meteo', 'gif', 'png', 'jpg']\n",
        "\n",
        "try:\n",
        "    # 2. Make an HTTP GET request to the URL with robust error handling and timeout\n",
        "    response = requests.get(IDEAM_RADAR_VIEWER_URL, allow_redirects=True, timeout=20)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    print(\"Successfully fetched IDEAM Radar Viewer page.\")\n",
        "\n",
        "    # 3. Parse the HTML content of the response\n",
        "    ideam_radar_viewer_soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(f\"Title of the page: {ideam_radar_viewer_soup.title.string if ideam_radar_viewer_soup.title else 'No title found'}\")\n",
        "\n",
        "    # 4. Inspect the HTML for <img> or <iframe> tags\n",
        "    print(\"\\n--- Inspecting for potential radar image/iframe sources ---\")\n",
        "    for tag in ideam_radar_viewer_soup.find_all(['img', 'iframe']):\n",
        "        if 'src' in tag.attrs:\n",
        "            src = tag['src']\n",
        "            # Construct absolute URL\n",
        "            full_src = requests.compat.urljoin(IDEAM_RADAR_VIEWER_URL, src)\n",
        "\n",
        "            # Check for keywords in the URL or surrounding text\n",
        "            if any(kw in full_src.lower() for kw in image_keywords):\n",
        "                radar_image_urls.append(full_src)\n",
        "            # Also check text content around the tag for descriptive titles\n",
        "            elif tag.name == 'iframe':\n",
        "                # Check previous/next sibling text, or parent's text\n",
        "                if tag.previous_sibling and isinstance(tag.previous_sibling, str) and any(kw in tag.previous_sibling.lower() for kw in image_keywords):\n",
        "                    radar_image_urls.append(full_src)\n",
        "                elif tag.next_sibling and isinstance(tag.next_sibling, str) and any(kw in tag.next_sibling.lower() for kw in image_keywords):\n",
        "                    radar_image_urls.append(full_src)\n",
        "                # A more general approach would involve searching the parent's text or checking title/alt attributes\n",
        "                elif 'title' in tag.attrs and any(kw in tag['title'].lower() for kw in image_keywords):\n",
        "                    radar_image_urls.append(full_src)\n",
        "\n",
        "    # Remove duplicates\n",
        "    radar_image_urls = list(set(radar_image_urls))\n",
        "\n",
        "    # 5. List potential radar image URLs\n",
        "    if radar_image_urls:\n",
        "        print(f\"\\nFound {len(radar_image_urls)} potential radar image/iframe URLs:\")\n",
        "        for url in radar_image_urls:\n",
        "            print(f\"- {url}\")\n",
        "\n",
        "        # 6. If URLs are found, attempt to download the most promising one\n",
        "        # Prioritize URLs that explicitly mention 'radar' and common image formats\n",
        "        prioritized_urls = sorted(radar_image_urls, key=lambda x: ('radar' not in x.lower(), '.gif' not in x.lower(), '.png' not in x.lower(), '.jpg' not in x.lower()))\n",
        "\n",
        "        ideam_radar_image_content = None\n",
        "        if prioritized_urls:\n",
        "            most_promising_url = prioritized_urls[0]\n",
        "            print(f\"\\nAttempting to download content from the most promising URL: {most_promising_url}\")\n",
        "            try:\n",
        "                response_image = requests.get(most_promising_url, allow_redirects=True, timeout=30)\n",
        "                response_image.raise_for_status()\n",
        "\n",
        "                content_type = response_image.headers.get('Content-Type', '').lower()\n",
        "                print(f\"Downloaded content type: {content_type}\")\n",
        "\n",
        "                if 'image' in content_type or 'application/octet-stream' in content_type:\n",
        "                    ideam_radar_image_content = response_image.content # Store raw bytes\n",
        "                    print(f\"Successfully downloaded image content. Size: {len(ideam_radar_image_content)} bytes.\")\n",
        "                elif 'text' in content_type or 'html' in content_type:\n",
        "                    ideam_radar_image_content = response_image.text # Store as text for inspection\n",
        "                    print(f\"Downloaded HTML/text content (not a direct image). Size: {len(ideam_radar_image_content)} bytes.\")\n",
        "                    print(f\"First 500 characters of content:\\n{ideam_radar_image_content[:500]}\")\n",
        "                else:\n",
        "                    ideam_radar_image_content = response_image.content\n",
        "                    print(f\"Downloaded content of unknown type. Size: {len(ideam_radar_image_content)} bytes.\")\n",
        "\n",
        "            except requests.exceptions.Timeout:\n",
        "                print(f\"Error: Request to {most_promising_url} timed out after 30 seconds.\")\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error fetching content from {most_promising_url}: {e}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred during download: {e}\")\n",
        "\n",
        "        if ideam_radar_image_content:\n",
        "            print(\"IDEAM radar image/content stored in 'ideam_radar_image_content'.\")\n",
        "        else:\n",
        "            print(\"Failed to obtain IDEAM radar image/content from promising URL.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No potential radar image URLs were found on the page.\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to {IDEAM_RADAR_VIEWER_URL} timed out after 20 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching IDEAM Radar Viewer page: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during processing: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_c_WCTBxvYU6"
      },
      "source": [
        "## Limpieza y Unificación de Datos (Meteoblue, Meteosource, SIATA)\n",
        "\n",
        "### Subtask:\n",
        "Limpiar, transformar y unificar los datos obtenidos de Meteoblue, Meteosource y SIATA en un DataFrame de Pandas común."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSJVL7_7sxUO"
      },
      "source": [
        "**Reasoning**:\n",
        "El código anterior fallaba porque el DataFrame 'daily' de Meteosource tenía una columna 'day' para las fechas en lugar de 'date'. Corregiré esto actualizando la referencia del nombre de la columna a 'day' para convertir correctamente la columna de fecha a objetos datetime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "hzfA2HU7s5xW",
        "outputId": "4e9376d5-0dea-4455-9816-e10b1c7dffa7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'meteosource_raw_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m meteosource_hourly_df = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      8\u001b[39m meteosource_daily_df = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmeteosource_raw_data\u001b[49m:\n\u001b[32m     11\u001b[39m     \u001b[38;5;66;03m# 1. Process 'current' data\u001b[39;00m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcurrent\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m meteosource_raw_data:\n\u001b[32m     13\u001b[39m         current_data = meteosource_raw_data[\u001b[33m'\u001b[39m\u001b[33mcurrent\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[31mNameError\u001b[39m: name 'meteosource_raw_data' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# --- Extracting and processing Meteosource data ---\n",
        "\n",
        "meteosource_current_df = None\n",
        "meteosource_hourly_df = None\n",
        "meteosource_daily_df = None\n",
        "\n",
        "if meteosource_raw_data:\n",
        "    # 1. Process 'current' data\n",
        "    if 'current' in meteosource_raw_data:\n",
        "        current_data = meteosource_raw_data['current']\n",
        "        # Flatten nested dictionaries like 'wind' and 'precipitation'\n",
        "        flattened_current = {}\n",
        "        for key, value in current_data.items():\n",
        "            if isinstance(value, dict):\n",
        "                for sub_key, sub_value in value.items():\n",
        "                    flattened_current[f\"{key}_{sub_key}\"] = sub_value\n",
        "            else:\n",
        "                flattened_current[key] = value\n",
        "        meteosource_current_df = pd.DataFrame([flattened_current])\n",
        "        # Add a timestamp for the current data\n",
        "        meteosource_current_df['date'] = pd.to_datetime(meteosource_raw_data['hourly']['data'][0]['date'] if 'hourly' in meteosource_raw_data and meteosource_raw_data['hourly']['data'] else pd.Timestamp.now())\n",
        "        print(\"Meteosource current data extracted and converted to DataFrame.\")\n",
        "        print(\"First 5 rows of meteosource_current_df:\")\n",
        "        print(meteosource_current_df.head())\n",
        "        print(\"\\nDataFrame Info for meteosource_current_df:\")\n",
        "        meteosource_current_df.info()\n",
        "\n",
        "    # 2. Process 'hourly' data\n",
        "    if 'hourly' in meteosource_raw_data and meteosource_raw_data['hourly']['data']:\n",
        "        meteosource_hourly_df = pd.DataFrame(meteosource_raw_data['hourly']['data'])\n",
        "        meteosource_hourly_df['date'] = pd.to_datetime(meteosource_hourly_df['date'])\n",
        "        print(\"\\n Meteosource hourly data extracted and converted to DataFrame.\")\n",
        "        print(\"First 5 rows of meteosource_hourly_df:\")\n",
        "        print(meteosource_hourly_df.head())\n",
        "        print(\"\\nDataFrame Info for meteosource_hourly_df:\")\n",
        "        meteosource_hourly_df.info()\n",
        "\n",
        "    # 3. Process 'daily' data\n",
        "    if 'daily' in meteosource_raw_data and meteosource_raw_data['daily']['data']:\n",
        "        meteosource_daily_df = pd.DataFrame(meteosource_raw_data['daily']['data'])\n",
        "        # Corrected: 'day' column is the date in daily data, not 'date'\n",
        "        meteosource_daily_df['date'] = pd.to_datetime(meteosource_daily_df['day'])\n",
        "        print(\"\\nMeteosource daily data extracted and converted to DataFrame.\")\n",
        "        print(\"First 5 rows of meteosource_daily_df:\")\n",
        "        print(meteosource_daily_df.head())\n",
        "        print(\"\\nDataFrame Info for meteosource_daily_df:\")\n",
        "        meteosource_daily_df.info()\n",
        "\n",
        "else:\n",
        "    print(\"No raw Meteosource data available to process.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ea308ec"
      },
      "source": [
        "### Obtener datos de Meteosource API\n",
        "\n",
        "**Subtask:** Utilizar la clave API de Meteosource y las coordenadas de Medellín para obtener datos meteorológicos de la API de Meteosource."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36466f19",
        "outputId": "4bfa6bd0-5095-42b9-96b8-a577c77cb39c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to fetch data from Meteosource API for Latitude: 6.2442, Longitude: -75.5812\n",
            "Successfully fetched data from Meteosource API.\n",
            "Meteosource raw data keys: dict_keys(['lat', 'lon', 'elevation', 'timezone', 'units', 'current', 'hourly', 'daily'])\n",
            "Meteosource raw data has been stored in 'meterosource_raw_data'.\n"
          ]
        }
      ],
      "source": [
        "# Replace 'YOUR_METEOSOURCE_API_KEY' with your actual Meteosource API key\n",
        "METEOSOURCE_API_KEY = \"8o3g3rf2mjb82r8zxb6vjhvo556beidj0xk1h9p6\"\n",
        "\n",
        "# Define the API endpoint and parameters for Medellín\n",
        "METEOSOURCE_BASE_URL = \"https://www.meteosource.com/api/v1/free/point\"\n",
        "\n",
        "# Assuming LATITUDE and LONGITUDE are already defined from Meteoblue setup\n",
        "LATITUDE = 6.2442\n",
        "LONGITUDE = -75.5812\n",
        "\n",
        "params = {\n",
        "    \"lat\": LATITUDE,\n",
        "    \"lon\": LONGITUDE,\n",
        "    \"sections\": \"current,hourly,daily\", # Request current, hourly, and daily data\n",
        "    \"timezone\": \"America/Bogota\",\n",
        "    \"language\": \"en\",\n",
        "    \"units\": \"metric\", # Use metric units\n",
        "    \"key\": METEOSOURCE_API_KEY\n",
        "}\n",
        "\n",
        "print(f\"Attempting to fetch data from Meteosource API for Latitude: {LATITUDE}, Longitude: {LONGITUDE}\")\n",
        "\n",
        "meterosource_raw_data = None\n",
        "\n",
        "try:\n",
        "    response = requests.get(METEOSOURCE_BASE_URL, params=params, timeout=30)\n",
        "    response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "    meterosource_raw_data = response.json()\n",
        "    print(\"Successfully fetched data from Meteosource API.\")\n",
        "    print(f\"Meteosource raw data keys: {meterosource_raw_data.keys()}\")\n",
        "\n",
        "except requests.exceptions.Timeout:\n",
        "    print(f\"Error: Request to Meteosource API timed out after 30 seconds.\")\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching data from Meteosource API: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during Meteosource API processing: {e}\")\n",
        "\n",
        "if meterosource_raw_data:\n",
        "    print(\"Meteosource raw data has been stored in 'meterosource_raw_data'.\")\n",
        "else:\n",
        "    print(\"Failed to retrieve Meteosource data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "bg-GLWPwtAae",
        "outputId": "712790c2-d5a7-4c9d-f4fc-5feb44a9482c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Inspecting Meteoblue DataFrame (meteoblue_df) ---\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'meteoblue_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-251185125.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Inspecting Meteoblue DataFrame (meteoblue_df) ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmeteoblue_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeteoblue_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeteoblue_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'meteoblue_df' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Inspecting Meteoblue DataFrame (meteoblue_df) ---\")\n",
        "if meteoblue_df is not None:\n",
        "    print(meteoblue_df.head())\n",
        "    print(meteoblue_df.info())\n",
        "else:\n",
        "    print(\"meteoblue_df is not available.\")\n",
        "\n",
        "print(\"\\n--- Inspecting Meteosource Current DataFrame (meteosource_current_df) ---\")\n",
        "if meteosource_current_df is not None:\n",
        "    print(meteosource_current_df.head())\n",
        "    print(meteosource_current_df.info())\n",
        "else:\n",
        "    print(\"meteosource_current_df is not available.\")\n",
        "\n",
        "print(\"\\n--- Inspecting Meteosource Hourly DataFrame (meteosource_hourly_df) ---\")\n",
        "if meteosource_hourly_df is not None:\n",
        "    print(meteosource_hourly_df.head())\n",
        "    print(meteosource_hourly_df.info())\n",
        "else:\n",
        "    print(\"meteosource_hourly_df is not available.\")\n",
        "\n",
        "print(\"\\n--- Inspecting Meteosource Daily DataFrame (meteosource_daily_df) ---\")\n",
        "if meteosource_daily_df is not None:\n",
        "    print(meteosource_daily_df.head())\n",
        "    print(meteosource_daily_df.info())\n",
        "else:\n",
        "    print(\"meteosource_daily_df is not available.\")\n",
        "\n",
        "print(\"\\n--- Inspecting SIATA Precipitation DataFrame (siata_precipitacion_data) ---\")\n",
        "if siata_precipitacion_data is not None:\n",
        "    print(siata_precipitacion_data.head())\n",
        "    print(siata_precipitacion_data.info())\n",
        "else:\n",
        "    print(\"siata_precipitacion_data is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fT3TyYYOtIvf",
        "outputId": "5b0dbcea-06df-486c-86e9-1a0ff75ac58e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Flattening of nested Meteosource columns complete.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# --- Flattening nested columns in Meteosource DataFrames ---\n",
        "\n",
        "# Flatten 'wind', 'precipitation', 'cloud_cover' in meteosource_hourly_df\n",
        "if meteosource_hourly_df is not None:\n",
        "    print(\"Flattening nested columns in meteosource_hourly_df...\")\n",
        "    # Wind data\n",
        "    if 'wind' in meteosource_hourly_df.columns:\n",
        "        meteosource_hourly_df = pd.concat([meteosource_hourly_df.drop('wind', axis=1),\n",
        "                                           meteosource_hourly_df['wind'].apply(pd.Series).add_prefix('wind_')], axis=1)\n",
        "    # Precipitation data\n",
        "    if 'precipitation' in meteosource_hourly_df.columns:\n",
        "        meteosource_hourly_df = pd.concat([meteosource_hourly_df.drop('precipitation', axis=1),\n",
        "                                           meteosource_hourly_df['precipitation'].apply(pd.Series).add_prefix('precipitation_')], axis=1)\n",
        "    # Cloud cover data\n",
        "    if 'cloud_cover' in meteosource_hourly_df.columns:\n",
        "        meteosource_hourly_df = pd.concat([meteosource_hourly_df.drop('cloud_cover', axis=1),\n",
        "                                           meteosource_hourly_df['cloud_cover'].apply(pd.Series).add_prefix('cloud_cover_')], axis=1)\n",
        "\n",
        "    print(\"Meteosource hourly DataFrame after flattening:\")\n",
        "    print(meteosource_hourly_df.head())\n",
        "    print(meteosource_hourly_df.info())\n",
        "\n",
        "# Flatten 'all_day' and similar in meteosource_daily_df (which contain nested weather details)\n",
        "if meteosource_daily_df is not None:\n",
        "    print(\"\\nFlattening nested columns in meteosource_daily_df...\")\n",
        "\n",
        "    # The 'all_day' column in daily data contains detailed weather for the day.\n",
        "    # We will extract temperature_max, temperature_min, precipitation_total, and wind_speed from it.\n",
        "    if 'all_day' in meteosource_daily_df.columns:\n",
        "        temp_df = meteosource_daily_df['all_day'].apply(pd.Series)\n",
        "\n",
        "        # Extract temperature info directly from temp_df as 'temperature_max' and 'temperature_min' are already there\n",
        "        if 'temperature_max' in temp_df.columns:\n",
        "            meteosource_daily_df['temperature_max'] = temp_df['temperature_max']\n",
        "        if 'temperature_min' in temp_df.columns:\n",
        "            meteosource_daily_df['temperature_min'] = temp_df['temperature_min']\n",
        "\n",
        "        # Extract precipitation info\n",
        "        if 'precipitation' in temp_df.columns:\n",
        "            temp_precip_df = temp_df['precipitation'].apply(pd.Series)\n",
        "            if 'total' in temp_precip_df.columns:\n",
        "                meteosource_daily_df['precipitation_total'] = temp_precip_df['total']\n",
        "\n",
        "        # Extract wind speed (might be nested under wind as well)\n",
        "        if 'wind' in temp_df.columns:\n",
        "            temp_wind_df = temp_df['wind'].apply(pd.Series)\n",
        "            if 'speed' in temp_wind_df.columns:\n",
        "                meteosource_daily_df['wind_speed'] = temp_wind_df['speed']\n",
        "\n",
        "        # Drop the original nested 'all_day' column\n",
        "        meteosource_daily_df = meteosource_daily_df.drop(columns=['all_day'])\n",
        "\n",
        "    print(\"Meteosource daily DataFrame after flattening:\")\n",
        "    print(meteosource_daily_df.head())\n",
        "    print(meteosource_daily_df.info())\n",
        "\n",
        "\n",
        "print(\"\\nFlattening of nested Meteosource columns complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqxs544gtN8f"
      },
      "source": [
        "**Reasoning**:\n",
        "El código anterior falló porque `temp_df['temperature']` ya era una serie escalar, lo que convertía a `temp_temp_df` en un DataFrame con una sola columna `0`, y por lo tanto, `temp_temp_df['max']` generaba un KeyError. Los valores de `temperature_max` y `temperature_min` ya están disponibles directamente en `temp_df` gracias al aplanamiento de `all_day`. Corregiré esto accediendo directamente a estas columnas desde `temp_df`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "gT8L5A04tdBA",
        "outputId": "9f92e013-fee4-48d4-ab55-58d9ec3ec3ca"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'meteoblue_df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2111765927.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# --- Process Meteoblue DataFrame ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmeteoblue_df\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing Meteoblue DataFrame...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Rename columns to common names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'meteoblue_df' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define common column names for unification\n",
        "COMMON_COLUMNS = {\n",
        "    'date': 'date',\n",
        "    'time': 'date', # Meteoblue uses 'time' for date\n",
        "    'temperature_mean': 'avg_temperature_c',\n",
        "    'temperature': 'avg_temperature_c',\n",
        "    'temperature_instant': 'current_temperature_c',\n",
        "    'temperature_max': 'max_temperature_c',\n",
        "    'temperature_min': 'min_temperature_c',\n",
        "    'precipitation': 'total_precipitation_mm', # Meteoblue\n",
        "    'precipitation_total': 'total_precipitation_mm', # Meteosource\n",
        "    'windspeed_mean': 'avg_wind_speed_ms',\n",
        "    'wind_speed': 'avg_wind_speed_ms',\n",
        "    'windspeed_max': 'max_wind_speed_ms',\n",
        "    'windspeed_min': 'min_wind_speed_ms',\n",
        "    'relativehumidity_mean': 'avg_relative_humidity_percent',\n",
        "    'relativehumidity_max': 'max_relative_humidity_percent',\n",
        "    'relativehumidity_min': 'min_relative_humidity_percent',\n",
        "    'Acumulado Mes (mm)': 'total_precipitation_mm', # SIATA\n",
        "    'Estacion': 'station_id',\n",
        "    'Nombre': 'station_name',\n",
        "    'Municipio': 'municipality',\n",
        "    'Barrio': 'neighborhood'\n",
        "}\n",
        "\n",
        "# --- Process Meteoblue DataFrame ---\n",
        "if meteoblue_df is not None:\n",
        "    print(\"Processing Meteoblue DataFrame...\")\n",
        "    # Rename columns to common names\n",
        "    meteoblue_df = meteoblue_df.rename(columns=COMMON_COLUMNS)\n",
        "    # Select relevant columns for unification\n",
        "    meteoblue_df_cleaned = meteoblue_df[['date', 'current_temperature_c', 'max_temperature_c', 'min_temperature_c',\n",
        "                                           'total_precipitation_mm', 'avg_wind_speed_ms']].copy()\n",
        "    meteoblue_df_cleaned['source'] = 'Meteoblue'\n",
        "    # Ensure date is just a date for daily comparison\n",
        "    meteoblue_df_cleaned['date'] = meteoblue_df_cleaned['date'].dt.date\n",
        "    print(\"Meteoblue DataFrame processed.\")\n",
        "    print(meteoblue_df_cleaned.head())\n",
        "    print(meteoblue_df_cleaned.info())\n",
        "\n",
        "# --- Process Meteosource Current DataFrame ---\n",
        "if meteosource_current_df is not None:\n",
        "    print(\"\\nProcessing Meteosource Current DataFrame...\")\n",
        "    meteosource_current_df = meteosource_current_df.rename(columns=COMMON_COLUMNS)\n",
        "    # Ensure the 'date' column is datetime64[ns] before using .dt accessor\n",
        "    meteosource_current_df['date'] = pd.to_datetime(meteosource_current_df['date'])\n",
        "\n",
        "    # The 'date' column is datetime, convert to date only for daily comparison\n",
        "    meteosource_current_df['date'] = meteosource_current_df['date'].dt.date\n",
        "    # Corrected: use 'avg_temperature_c' as 'temperature' was mapped to it\n",
        "    meteosource_current_df_cleaned = meteosource_current_df[['date', 'avg_temperature_c',\n",
        "                                                             'avg_wind_speed_ms', 'total_precipitation_mm']].copy()\n",
        "    meteosource_current_df_cleaned['source'] = 'Meteosource_Current'\n",
        "    print(\"Meteosource Current DataFrame processed.\")\n",
        "    print(meteosource_current_df_cleaned.head())\n",
        "    print(meteosource_current_df_cleaned.info())\n",
        "\n",
        "# --- Process Meteosource Hourly DataFrame ---\n",
        "if meteosource_hourly_df is not None:\n",
        "    print(\"\\nProcessing Meteosource Hourly DataFrame...\")\n",
        "    meteosource_hourly_df = meteosource_hourly_df.rename(columns=COMMON_COLUMNS)\n",
        "    # Hourly data might need to be aggregated to daily or kept as is, depending on the target granularity.\n",
        "    # For now, let's keep it hourly, but extract some key daily aggregations if needed later.\n",
        "    # For direct unification, we'll aim for daily for now.\n",
        "    meteosource_hourly_df['date_only'] = meteosource_hourly_df['date'].dt.date\n",
        "    meteosource_hourly_df_cleaned = meteosource_hourly_df.groupby('date_only').agg(\n",
        "        avg_temperature_c=('avg_temperature_c', 'mean'),\n",
        "        max_temperature_c=('avg_temperature_c', 'max'),\n",
        "        min_temperature_c=('avg_temperature_c', 'min'),\n",
        "        total_precipitation_mm=('total_precipitation_mm', 'sum'),\n",
        "        avg_wind_speed_ms=('avg_wind_speed_ms', 'mean')\n",
        "    ).reset_index().rename(columns={'date_only': 'date'})\n",
        "    meteosource_hourly_df_cleaned['source'] = 'Meteosource_Hourly_Agg'\n",
        "    print(\"Meteosource Hourly DataFrame processed and aggregated.\")\n",
        "    print(meteosource_hourly_df_cleaned.head())\n",
        "    print(meteosource_hourly_df_cleaned.info())\n",
        "\n",
        "# --- Process Meteosource Daily DataFrame ---\n",
        "if meteosource_daily_df is not None:\n",
        "    print(\"\\nProcessing Meteosource Daily DataFrame...\")\n",
        "    meteosource_daily_df = meteosource_daily_df.rename(columns=COMMON_COLUMNS)\n",
        "    meteosource_daily_df_cleaned = meteosource_daily_df[['date', 'max_temperature_c', 'min_temperature_c',\n",
        "                                                            'total_precipitation_mm', 'avg_wind_speed_ms']].copy()\n",
        "    meteosource_daily_df_cleaned['source'] = 'Meteosource_Daily'\n",
        "    # Ensure date is just a date for daily comparison\n",
        "    meteosource_daily_df_cleaned['date'] = meteosource_daily_df_cleaned['date'].dt.date\n",
        "    print(\"Meteosource Daily DataFrame processed.\")\n",
        "    print(meteosource_daily_df_cleaned.head())\n",
        "    print(meteosource_daily_df_cleaned.info())\n",
        "\n",
        "# --- Process SIATA Precipitation DataFrame ---\n",
        "if siata_precipitacion_data is not None:\n",
        "    print(\"\\nProcessing SIATA Precipitation DataFrame...\")\n",
        "    siata_precipitacion_data = siata_precipitacion_data.rename(columns=COMMON_COLUMNS)\n",
        "    siata_precipitacion_data_cleaned = siata_precipitacion_data[['station_id', 'station_name', 'municipality',\n",
        "                                                                'neighborhood', 'total_precipitation_mm']].copy()\n",
        "    # SIATA data is not time-series in this file, it's station-based monthly accumulation.\n",
        "    # For unification with daily data, this will need special handling or be used as a separate dataset.\n",
        "    # For now, we will keep it as is, noting its different granularity.\n",
        "    siata_precipitacion_data_cleaned['source'] = 'SIATA_Monthly_Precipitation'\n",
        "    print(\"SIATA Precipitation DataFrame processed.\")\n",
        "    print(siata_precipitacion_data_cleaned.head())\n",
        "    print(siata_precipitacion_data_cleaned.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5pLP_sMzt5zC",
        "outputId": "1ca4cf75-03e6-496b-97ce-9e9fb23aa451"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'meteoblue_df_cleaned' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4192142256.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --- Prepare Meteoblue DataFrame ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmeteoblue_df_cleaned\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preparing Meteoblue for unification...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Use 'current_temperature_c' as 'avg_temperature_c' for daily view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'meteoblue_df_cleaned' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define target common columns for daily weather data unification\n",
        "target_daily_columns = [\n",
        "    'date',\n",
        "    'avg_temperature_c',\n",
        "    'max_temperature_c',\n",
        "    'min_temperature_c',\n",
        "    'total_precipitation_mm',\n",
        "    'avg_wind_speed_ms',\n",
        "    'source'\n",
        "]\n",
        "\n",
        "unified_weather_data = []\n",
        "\n",
        "# --- Prepare Meteoblue DataFrame ---\n",
        "if meteoblue_df_cleaned is not None:\n",
        "    print(\"Preparing Meteoblue for unification...\")\n",
        "    # Use 'current_temperature_c' as 'avg_temperature_c' for daily view\n",
        "    meteoblue_df_temp = meteoblue_df_cleaned.copy()\n",
        "    meteoblue_df_temp['avg_temperature_c'] = meteoblue_df_temp['current_temperature_c']\n",
        "    # Ensure all target columns exist, fill missing with NaN\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteoblue_df_temp.columns:\n",
        "            meteoblue_df_temp[col] = pd.NA\n",
        "    unified_weather_data.append(meteoblue_df_temp[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Current DataFrame ---\n",
        "if meteosource_current_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Current for unification...\")\n",
        "    meteosource_current_temp = meteosource_current_df_cleaned.copy()\n",
        "    # For current data, max/min temp might not be explicitly available for the full day.\n",
        "    # Fill them with avg_temperature_c for consistency in daily view, or NaN if preferred.\n",
        "    meteosource_current_temp['max_temperature_c'] = meteosource_current_temp['avg_temperature_c']\n",
        "    meteosource_current_temp['min_temperature_c'] = meteosource_current_temp['avg_temperature_c']\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_current_temp.columns:\n",
        "            meteosource_current_temp[col] = pd.NA\n",
        "    unified_weather_data.append(meteosource_current_temp[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Hourly Aggregated DataFrame ---\n",
        "if meteosource_hourly_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Hourly Aggregated for unification...\")\n",
        "    # Already has target columns after aggregation\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_hourly_df_cleaned.columns:\n",
        "            meteosource_hourly_df_cleaned[col] = pd.NA\n",
        "    unified_weather_data.append(meteosource_hourly_df_cleaned[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Daily DataFrame ---\n",
        "if meteosource_daily_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Daily for unification...\")\n",
        "    # Ensure all target columns exist, fill missing with NaN\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_daily_df_cleaned.columns:\n",
        "            meteosource_daily_df_cleaned[col] = pd.NA\n",
        "    unified_weather_data.append(meteosource_daily_df_cleaned[target_daily_columns])\n",
        "\n",
        "# --- Concatenate all prepared DataFrames ---\n",
        "if unified_weather_data:\n",
        "    print(\"Concatenating all daily weather dataframes...\")\n",
        "    unified_daily_weather_df = pd.concat(unified_weather_data, ignore_index=True)\n",
        "    unified_daily_weather_df['date'] = pd.to_datetime(unified_daily_weather_df['date'])\n",
        "    print(\"\\nUnified Daily Weather DataFrame created.\")\n",
        "    print(\"First 10 rows of unified_daily_weather_df:\")\n",
        "    print(unified_daily_weather_df.head(10))\n",
        "    print(\"\\nDataFrame Info for unified_daily_weather_df:\")\n",
        "    unified_daily_weather_df.info()\n",
        "else:\n",
        "    print(\"No weather data available for unification.\")\n",
        "\n",
        "\n",
        "print(\"\\n\\nSIATA Precipitation Data (kept separate due to different granularity):\")\n",
        "if siata_precipitacion_data_cleaned is not None:\n",
        "    print(siata_precipitacion_data_cleaned.head())\n",
        "    print(siata_precipitacion_data_cleaned.info())\n",
        "else:\n",
        "    print(\"SIATA precipitation data is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "jCZ9rK7Vt9Sc",
        "outputId": "4b4ee60e-0aa0-49aa-80e8-164d71f34d71"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'meteoblue_df_cleaned' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2166844890.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# --- Prepare Meteoblue DataFrame ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mmeteoblue_df_cleaned\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preparing Meteoblue for unification...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Use 'current_temperature_c' as 'avg_temperature_c' for daily view\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'meteoblue_df_cleaned' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define target common columns for daily weather data unification\n",
        "target_daily_columns = [\n",
        "    'date',\n",
        "    'avg_temperature_c',\n",
        "    'max_temperature_c',\n",
        "    'min_temperature_c',\n",
        "    'total_precipitation_mm',\n",
        "    'avg_wind_speed_ms',\n",
        "    'source'\n",
        "]\n",
        "\n",
        "unified_weather_data = []\n",
        "\n",
        "# --- Prepare Meteoblue DataFrame ---\n",
        "if meteoblue_df_cleaned is not None:\n",
        "    print(\"Preparing Meteoblue for unification...\")\n",
        "    # Use 'current_temperature_c' as 'avg_temperature_c' for daily view\n",
        "    meteoblue_df_temp = meteoblue_df_cleaned.copy()\n",
        "    meteoblue_df_temp['avg_temperature_c'] = meteoblue_df_temp['current_temperature_c']\n",
        "    # Ensure all target columns exist, fill missing with NaN\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteoblue_df_temp.columns:\n",
        "            meteoblue_df_temp[col] = pd.NA\n",
        "    unified_weather_data.append(meteoblue_df_temp[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Current DataFrame ---\n",
        "if meteosource_current_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Current for unification...\")\n",
        "    meteosource_current_temp = meteosource_current_df_cleaned.copy()\n",
        "    # For current data, max/min temp might not be explicitly available for the full day.\n",
        "    # Fill them with avg_temperature_c for consistency in daily view, or NaN if preferred.\n",
        "    meteosource_current_temp['max_temperature_c'] = meteosource_current_temp['avg_temperature_c']\n",
        "    meteosource_current_temp['min_temperature_c'] = meteosource_current_temp['avg_temperature_c']\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_current_temp.columns:\n",
        "            meteosource_current_temp[col] = pd.NA\n",
        "    unified_weather_data.append(meteosource_current_temp[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Hourly Aggregated DataFrame ---\n",
        "if meteosource_hourly_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Hourly Aggregated for unification...\")\n",
        "    # Already has target columns after aggregation\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_hourly_df_cleaned.columns:\n",
        "            meteosource_hourly_df_cleaned[col] = pd.NA\n",
        "    unified_weather_data.append(meteosource_hourly_df_cleaned[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Daily DataFrame ---\n",
        "if meteosource_daily_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Daily for unification...\")\n",
        "    # Ensure all target columns exist, fill missing with NaN\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_daily_df_cleaned.columns:\n",
        "            meteosource_daily_df_cleaned[col] = pd.NA\n",
        "    unified_weather_data.append(meteosource_daily_df_cleaned[target_daily_columns])\n",
        "\n",
        "# --- Concatenate all prepared DataFrames ---\n",
        "if unified_weather_data:\n",
        "    # Filter out any potentially empty DataFrames before concatenation\n",
        "    non_empty_dfs = [df for df in unified_weather_data if not df.empty]\n",
        "    if non_empty_dfs:\n",
        "        print(\"Concatenating all daily weather dataframes...\")\n",
        "        unified_daily_weather_df = pd.concat(non_empty_dfs, ignore_index=True)\n",
        "        unified_daily_weather_df['date'] = pd.to_datetime(unified_daily_weather_df['date'])\n",
        "        print(\"\\nUnified Daily Weather DataFrame created.\")\n",
        "        print(\"First 10 rows of unified_daily_weather_df:\")\n",
        "        print(unified_daily_weather_df.head(10))\n",
        "        print(\"\\nDataFrame Info for unified_daily_weather_df:\")\n",
        "        unified_daily_weather_df.info()\n",
        "    else:\n",
        "        print(\"All prepared DataFrames were empty. No data to unify.\")\n",
        "else:\n",
        "    print(\"No weather data available for unification.\")\n",
        "\n",
        "\n",
        "print(\"\\n\\nSIATA Precipitation Data (kept separate due to different granularity):\")\n",
        "if siata_precipitacion_data_cleaned is not None:\n",
        "    print(siata_precipitacion_data_cleaned.head())\n",
        "    print(siata_precipitacion_data_cleaned.info())\n",
        "else:\n",
        "    print(\"SIATA precipitation data is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRQP11XQuB26"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define target common columns for daily weather data unification\n",
        "target_daily_columns = [\n",
        "    'date',\n",
        "    'avg_temperature_c',\n",
        "    'max_temperature_c',\n",
        "    'min_temperature_c',\n",
        "    'total_precipitation_mm',\n",
        "    'avg_wind_speed_ms',\n",
        "    'source'\n",
        "]\n",
        "\n",
        "unified_weather_data = []\n",
        "\n",
        "# --- Prepare Meteoblue DataFrame ---\n",
        "if meteoblue_df_cleaned is not None:\n",
        "    print(\"Preparing Meteoblue for unification...\")\n",
        "    meteoblue_df_temp = meteoblue_df_cleaned.copy()\n",
        "    meteoblue_df_temp['avg_temperature_c'] = meteoblue_df_temp['current_temperature_c']\n",
        "\n",
        "    # Ensure all target columns exist and explicitly set nullable float dtype for numeric columns\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteoblue_df_temp.columns:\n",
        "            if col in ['avg_temperature_c', 'max_temperature_c', 'min_temperature_c', 'total_precipitation_mm', 'avg_wind_speed_ms']:\n",
        "                meteoblue_df_temp[col] = pd.Series(dtype=pd.Float64Dtype())\n",
        "            else:\n",
        "                meteoblue_df_temp[col] = pd.NA\n",
        "    # Cast existing numeric columns to nullable float to ensure consistency\n",
        "    for col in ['avg_temperature_c', 'max_temperature_c', 'min_temperature_c', 'total_precipitation_mm', 'avg_wind_speed_ms']:\n",
        "        if col in meteoblue_df_temp.columns: # Check if column exists after potential addition\n",
        "            meteoblue_df_temp[col] = meteoblue_df_temp[col].astype(pd.Float64Dtype())\n",
        "\n",
        "    unified_weather_data.append(meteoblue_df_temp[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Current DataFrame ---\n",
        "if meteosource_current_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Current for unification...\")\n",
        "    meteosource_current_temp = meteosource_current_df_cleaned.copy()\n",
        "    meteosource_current_temp['max_temperature_c'] = meteosource_current_temp['avg_temperature_c']\n",
        "    meteosource_current_temp['min_temperature_c'] = meteosource_current_temp['avg_temperature_c']\n",
        "\n",
        "    # Ensure all target columns exist and explicitly set nullable float dtype for numeric columns\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_current_temp.columns:\n",
        "            if col in ['avg_temperature_c', 'max_temperature_c', 'min_temperature_c', 'total_precipitation_mm', 'avg_wind_speed_ms']:\n",
        "                meteosource_current_temp[col] = pd.Series(dtype=pd.Float64Dtype())\n",
        "            else:\n",
        "                meteosource_current_temp[col] = pd.NA\n",
        "    # Cast existing numeric columns to nullable float\n",
        "    for col in ['avg_temperature_c', 'max_temperature_c', 'min_temperature_c', 'total_precipitation_mm', 'avg_wind_speed_ms']:\n",
        "        if col in meteosource_current_temp.columns: # Check if column exists after potential addition\n",
        "            meteosource_current_temp[col] = meteosource_current_temp[col].astype(pd.Float64Dtype())\n",
        "\n",
        "    unified_weather_data.append(meteosource_current_temp[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Hourly Aggregated DataFrame ---\n",
        "if meteosource_hourly_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Hourly Aggregated for unification...\")\n",
        "    # Ensure all target columns exist and explicitly set nullable float dtype for numeric columns\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_hourly_df_cleaned.columns:\n",
        "            if col in ['avg_temperature_c', 'max_temperature_c', 'min_temperature_c', 'total_precipitation_mm', 'avg_wind_speed_ms']:\n",
        "                meteosource_hourly_df_cleaned[col] = pd.Series(dtype=pd.Float64Dtype())\n",
        "            else:\n",
        "                meteosource_hourly_df_cleaned[col] = pd.NA\n",
        "    # Cast existing numeric columns to nullable float\n",
        "    for col in ['avg_temperature_c', 'max_temperature_c', 'min_temperature_c', 'total_precipitation_mm', 'avg_wind_speed_ms']:\n",
        "        if col in meteosource_hourly_df_cleaned.columns: # Check if column exists after potential addition\n",
        "            meteosource_hourly_df_cleaned[col] = meteosource_hourly_df_cleaned[col].astype(pd.Float64Dtype())\n",
        "\n",
        "    unified_weather_data.append(meteosource_hourly_df_cleaned[target_daily_columns])\n",
        "\n",
        "# --- Prepare Meteosource Daily DataFrame ---\n",
        "if meteosource_daily_df_cleaned is not None:\n",
        "    print(\"Preparing Meteosource Daily for unification...\")\n",
        "    # Ensure all target columns exist and explicitly set nullable float dtype for numeric columns\n",
        "    for col in target_daily_columns:\n",
        "        if col not in meteosource_daily_df_cleaned.columns:\n",
        "            if col in ['avg_temperature_c', 'max_temperature_c', 'min_temperature_c', 'total_precipitation_mm', 'avg_wind_speed_ms']:\n",
        "                meteosource_daily_df_cleaned[col] = pd.Series(dtype=pd.Float64Dtype())\n",
        "            else:\n",
        "                meteosource_daily_df_cleaned[col] = pd.NA\n",
        "    # Cast existing numeric columns to nullable float\n",
        "    for col in ['avg_temperature_c', 'max_temperature_c', 'min_temperature_c', 'total_precipitation_mm', 'avg_wind_speed_ms']:\n",
        "        if col in meteosource_daily_df_cleaned.columns: # Check if column exists after potential addition\n",
        "            meteosource_daily_df_cleaned[col] = meteosource_daily_df_cleaned[col].astype(pd.Float64Dtype())\n",
        "\n",
        "    unified_weather_data.append(meteosource_daily_df_cleaned[target_daily_columns])\n",
        "\n",
        "# --- Concatenate all prepared DataFrames ---\n",
        "if unified_weather_data:\n",
        "    # Filter out any potentially empty DataFrames before concatenation\n",
        "    non_empty_dfs = [df for df in unified_weather_data if not df.empty]\n",
        "    if non_empty_dfs:\n",
        "        print(\"Concatenating all daily weather dataframes...\")\n",
        "        unified_daily_weather_df = pd.concat(non_empty_dfs, ignore_index=True)\n",
        "        unified_daily_weather_df['date'] = pd.to_datetime(unified_daily_weather_df['date'])\n",
        "        print(\"\\nUnified Daily Weather DataFrame created.\")\n",
        "        print(\"First 10 rows of unified_daily_weather_df:\")\n",
        "        print(unified_daily_weather_df.head(10))\n",
        "        print(\"\\nDataFrame Info for unified_daily_weather_df:\")\n",
        "        unified_daily_weather_df.info()\n",
        "    else:\n",
        "        print(\"All prepared DataFrames were empty. No data to unify.\")\n",
        "else:\n",
        "    print(\"No weather data available for unification.\")\n",
        "\n",
        "\n",
        "print(\"\\n\\nSIATA Precipitation Data (kept separate due to different granularity):\")\n",
        "if siata_precipitacion_data_cleaned is not None:\n",
        "    print(siata_precipitacion_data_cleaned.head())\n",
        "    print(siata_precipitacion_data_cleaned.info())\n",
        "else:\n",
        "    print(\"SIATA precipitation data is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pKAxdNwuFMV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define filenames\n",
        "UNIFIED_DAILY_WEATHER_FILENAME = \"unified_daily_weather_data.csv\"\n",
        "SIATA_PRECIPITATION_FILENAME = \"siata_precipitation_data.csv\"\n",
        "\n",
        "# 1. Save unified_daily_weather_df\n",
        "if 'unified_daily_weather_df' in locals() and unified_daily_weather_df is not None and not unified_daily_weather_df.empty:\n",
        "    unified_daily_weather_df.to_csv(UNIFIED_DAILY_WEATHER_FILENAME, index=False)\n",
        "    print(f\"Unified daily weather data saved to '{UNIFIED_DAILY_WEATHER_FILENAME}' in the current directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"Unified daily weather DataFrame is not available or is empty, skipping save.\")\n",
        "\n",
        "# 2. Save siata_precipitacion_data_cleaned\n",
        "if 'siata_precipitacion_data_cleaned' in locals() and siata_precipitacion_data_cleaned is not None and not siata_precipitacion_data_cleaned.empty:\n",
        "    siata_precipitacion_data_cleaned.to_csv(SIATA_PRECIPITATION_FILENAME, index=False)\n",
        "    print(f\"SIATA precipitation data saved to '{SIATA_PRECIPITATION_FILENAME}' in the current directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(\"SIATA precipitation data DataFrame is not available or is empty, skipping save.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpLdblG7sKHi"
      },
      "source": [
        "## Obtener Datos de Meteosource\n",
        "\n",
        "### Subtask:\n",
        "Desarrollar código Python para obtener datos meteorológicos de Meteosource API para Medellín y su Área Metropolitana. (Necesitaremos la API key de Meteosource para esta implementación)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32j-QSEtnkXe"
      },
      "outputs": [],
      "source": [
        "API_KEY = \"Z2AnKNoxLJul08UQ\"\n",
        "print(\"Meteoblue API key defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUbMgfJMoIhD"
      },
      "source": [
        "**Reasoning**: A continuación, especificaré las coordenadas de Medellín según las instrucciones, las cuales son cruciales para realizar solicitudes a la API de Meteoblue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dI1SAw8RoBN8"
      },
      "outputs": [],
      "source": [
        "LATITUDE = 6.2442\n",
        "LONGITUDE = -75.5812\n",
        "\n",
        "print(f\"Coordinates for Medellín defined: Latitude={LATITUDE}, Longitude={LONGITUDE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6zVuP5aoTS1"
      },
      "source": [
        "**Reasoning**: Una vez definidos la clave API y las coordenadas, debo construir la URL para la llamada a la API de Meteoblue. Esto implica especificar la URL base, los parámetros de la clave API, las coordenadas y las variables meteorológicas específicas que se recuperarán, tal como se indica en las instrucciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYjLroAFod3w"
      },
      "outputs": [],
      "source": [
        "BASE_URL = \"http://my.meteoblue.com/packages/basic-day\"\n",
        "\n",
        "# Define the parameters for the API request\n",
        "# We'll request temperature, precipitation, wind speed, and relative humidity for now\n",
        "params = {\n",
        "    \"apikey\": API_KEY,\n",
        "    \"lat\": LATITUDE,\n",
        "    \"lon\": LONGITUDE,\n",
        "    \"asl\": 1500, # Approximate altitude for Medellín (meters above sea level)\n",
        "    \"tz\": \"America/Bogota\", # Timezone for Medellín\n",
        "    \"format\": \"json\",\n",
        "    \"temperature\": \"2m_temp,max_temp,min_temp\",\n",
        "    \"precipitation\": \"total_precip\",\n",
        "    \"wind\": \"wind_speed_10m\",\n",
        "    \"relativehumidity\": \"relative_humidity_2m\"\n",
        "}\n",
        "\n",
        "print(f\"Meteoblue API base URL: {BASE_URL}\")\n",
        "print(\"API parameters defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vV6FVlnol81"
      },
      "source": [
        "**Reasoning**: [texto del enlace](https://)Con la URL base y los parámetros definidos, realizaré una solicitud HTTP GET a la API de Meteoblue para obtener los datos meteorológicos. Esto se corresponde con la instrucción 4: «Realiza una solicitud HTTP GET a la URL de la API de Meteoblue para obtener los datos»."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqrq8xkdo-VC"
      },
      "outputs": [],
      "source": [
        "# Make the API request again with corrected parameters\n",
        "import requests\n",
        "\n",
        "response = requests.get(BASE_URL, params=params)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    print(\"Successfully fetched data from Meteoblue API with corrected parameters.\")\n",
        "    meteoblue_raw_data = response.json()\n",
        "    print(f\"Data keys received: {meteoblue_raw_data.keys()}\")\n",
        "else:\n",
        "    print(f\"Error fetching data with corrected parameters: {response.status_code} - {response.text}\")\n",
        "    meteoblue_raw_data = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCdayI7upHN8"
      },
      "source": [
        "**Reasoning**: El paso anterior recuperó y analizó correctamente la respuesta JSON en `meteoblue_raw_data`. Ahora, según la instrucción 6, confirmaré que los datos sin procesar se almacenan como un diccionario y mostraré sus claves y una muestra de los datos para facilitar su posterior manipulación y limpieza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaIzB3_FpN4j"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "if meteoblue_raw_data:\n",
        "    print(\"Meteoblue raw data successfully obtained and parsed as a dictionary.\")\n",
        "    print(f\"Top-level keys in meteoblue_raw_data: {meteoblue_raw_data.keys()}\")\n",
        "\n",
        "    # Display a sample of the 'data_day' key, which typically contains the weather data\n",
        "    if 'data_day' in meteoblue_raw_data:\n",
        "        data_day_content = meteoblue_raw_data['data_day']\n",
        "        if isinstance(data_day_content, list):\n",
        "            print(\"\\nSample of 'data_day' (first 2 entries):\")\n",
        "            # Safely slice the list; list[:2] works even for lists with fewer than 2 elements\n",
        "            print(json.dumps(data_day_content[:2], indent=2))\n",
        "        else:\n",
        "            print(f\"\\n'data_day' content is not a list. Type: {type(data_day_content)}. Printing full content for inspection:\")\n",
        "            # Attempt to dump the content even if not a list, for debugging\n",
        "            print(json.dumps(data_day_content, indent=2))\n",
        "\n",
        "    if 'metadata' in meteoblue_raw_data:\n",
        "        print(\"\\nMetadata information:\")\n",
        "        print(json.dumps(meteoblue_raw_data['metadata'], indent=2))\n",
        "else:\n",
        "    print(\"No raw data available from Meteoblue API.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScBj6LHbpq9e"
      },
      "source": [
        "**Reasoning**: La clave `data_day` contiene un diccionario donde cada clave representa una variable meteorológica y su valor es una lista de observaciones. Para facilitar su manipulación y limpieza, convertiré este diccionario en un DataFrame de Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6TkoP86pyfs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6dab3be6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "if meteoblue_raw_data and 'data_day' in meteoblue_raw_data:\n",
        "    # Convert the 'data_day' dictionary into a Pandas DataFrame\n",
        "    meteoblue_df = pd.DataFrame(meteoblue_raw_data['data_day'])\n",
        "\n",
        "    # Convert the 'time' column to datetime objects\n",
        "    meteoblue_df['time'] = pd.to_datetime(meteoblue_df['time'])\n",
        "\n",
        "    print(\"Meteoblue data successfully loaded into a Pandas DataFrame.\")\n",
        "    print(\"First 5 rows of the DataFrame:\")\n",
        "    print(meteoblue_df.head())\n",
        "    print(\"\\nDataFrame Info:\")\n",
        "    meteoblue_df.info()\n",
        "else:\n",
        "    print(\"No 'data_day' found in meteoblue_raw_data or data is empty.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
